@ARTICLE{Bayes1991-uo,
  title    = "An essay towards solving a problem in the doctrine of chances.
              1763",
  author   = "Bayes, T",
  year     =  1763,
  language = "en"
}

@ARTICLE{Blei2017-kg,
  title     = "Variational inference: A review for statisticians",
  author    = "Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D",
  abstract  = "One of the core problems of modern statistics is to approximate
               difficult-to-compute probability densities. This problem is
               especially important in Bayesian statistics, which frames all
               inference about unknown quantities as a calculation involving
               the posterior density. In this article, we review variational
               inference (VI), a method from machine learning that approximates
               probability densities through optimization. VI has been used in
               many applications and tends to be faster than classical methods,
               such as Markov chain Monte Carlo sampling. The idea behind VI is
               to first posit a family of densities and then to find a member
               of that family which is close to the target density. Closeness
               is measured by Kullback--Leibler divergence. We review the ideas
               behind mean-field variational inference, discuss the special
               case of VI applied to exponential family models, present a full
               example with a Bayesian mixture of Gaussians, and derive a
               variant that uses stochastic optimization to scale up to massive
               data. We discuss modern research in VI and highlight important
               open problems. VI is powerful, but it is not yet well
               understood. Our hope in writing this article is to catalyze
               statistical research on this class of algorithms. Supplementary
               materials for this article are available online.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Informa UK Limited",
  volume    =  112,
  number    =  518,
  pages     = "859--877",
  month     =  apr,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Caldwell2009-bw,
  title     = "{BAT} -- The Bayesian analysis toolkit",
  author    = "Caldwell, Allen and Koll{\'a}r, Daniel and Kr{\"o}ninger, Kevin",
  abstract  = "We describe the development of a new toolkit for data analysis.
               The analysis package is based on Bayes' Theorem, and is realized
               with the use of Markov Chain Monte Carlo. This gives access to
               the full posterior probability distribution. Parameter
               estimation, limit setting and uncertainty propagation are
               implemented in a straightforward manner.",
  journal   = "Comput. Phys. Commun.",
  publisher = "Elsevier BV",
  volume    =  180,
  number    =  11,
  pages     = "2197--2209",
  month     =  nov,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Carpenter2017-ul,
  title     = "Stan: A probabilistic programming language",
  author    = "Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and
               Lee, Daniel and Goodrich, Ben and Betancourt, Michael and
               Brubaker, Marcus A and Guo, Jiqiang and Li, Peter and Riddell,
               Allen",
  abstract  = "Stan is a probabilistic programming language for specifying
               statistical models. A Stan program imperatively defines a log
               probability function over parameters conditioned on specified
               data and constants. As of version 2.14.0, Stan provides full
               Bayesian inference for continuous-variable models through Markov
               chain Monte Carlo methods such as the No-U-Turn sampler, an
               adaptive form of Hamiltonian Monte Carlo sampling. Penalized
               maximum likelihood estimates are calculated using optimization
               methods such as the limited memory
               Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a
               platform for computing log densities and their gradients and
               Hessians, which can be used in alternative algorithms such as
               variational Bayes, expectation propagation, and marginal
               inference using approximate integration. To this end, Stan is
               set up so that the densities, gradients, and Hessians, along
               with intermediate quantities of the algorithm such as acceptance
               probabilities, are easily accessible. Stan can be called from
               the command line using the cmdstan package, through R using the
               rstan package, and through Python using the pystan package. All
               three interfaces support sampling and optimization-based
               inference with diagnostics and posterior analysis. rstan and
               pystan also provide access to log probabilities, gradients,
               Hessians, parameter transforms, and specialized plotting.",
  journal   = "J. Stat. Softw.",
  publisher = "Foundation for Open Access Statistic",
  volume    =  76,
  number    =  1,
  month     =  jan,
  year      =  2017,
  keywords  = "Bayesian inference; Stan; algorithmic differentiation;
               probabilistic program",
  language  = "en"
}

@ARTICLE{Gabry2019-qz,
  title     = "Visualization in Bayesian workflow",
  author    = "Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and
               Betancourt, Michael and Gelman, Andrew",
  abstract  = "AbstractBayesian data analysis is about more than just computing
               a posterior distribution, and Bayesian visualization is about
               more than trace plots of Markov chains. Practical Bayesian data
               analysis, like all data analysis, is an iterative process of
               model building, inference, model checking and evaluation, and
               model expansion. Visualization is helpful in each of these
               stages of the Bayesian workflow and it is indispensable when
               drawing inferences from the types of modern, high dimensional
               models that are used by applied researchers.",
  journal   = "J. R. Stat. Soc. Ser. A Stat. Soc.",
  publisher = "Oxford University Press (OUP)",
  volume    =  182,
  number    =  2,
  pages     = "389--402",
  month     =  feb,
  year      =  2019,
  copyright = "https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model",
  language  = "en"
}

@BOOK{Gelman2013-aj,
  title     = "Bayesian Data Analysis",
  author    = "Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson,
               David B and Vehtari, Aki and Rubin, Donald B",
  publisher = "Chapman and Hall/CRC",
  month     =  nov,
  year      =  2013
}

@MISC{noauthor_2021-ow,
  title        = "What is linear regression?",
  abstract     = "Linear regression is an analytics procedure that can generate
                  predictions by using an easily interpreted mathematical
                  formula.",
  month        =  aug,
  year         =  2021,
  howpublished = "\url{https://www.ibm.com/topics/linear-regression}",
  note         = "Accessed: 2024-9-16",
  language     = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Koehrsen2018-ff,
  title        = "Introduction to Bayesian linear regression",
  booktitle    = "Towards Data Science",
  author       = "Koehrsen, Will",
  abstract     = "The Bayesian vs Frequentist debate is one of those academic
                  arguments that I find more interesting to watch than engage
                  in. Rather than enthusiastically jump in on one side, I think
                  it's moreâ€¦",
  month        =  apr,
  year         =  2018,
  howpublished = "\url{https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7}",
  note         = "Accessed: 2024-9-16",
  language     = "en"
}

@ARTICLE{Van_de_Schoot2021-hd,
  title     = "Bayesian statistics and modelling",
  author    = "van de Schoot, Rens and Depaoli, Sarah and King, Ruth and
               Kramer, Bianca and M{\"a}rtens, Kaspar and Tadesse, Mahlet G and
               Vannucci, Marina and Gelman, Andrew and Veen, Duco and
               Willemsen, Joukje and Yau, Christopher",
  journal   = "Nat. Rev. Methods Primers",
  publisher = "Springer Science and Business Media LLC",
  volume    =  1,
  number    =  1,
  month     =  jan,
  year      =  2021,
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en"
}

@ARTICLE{Zyphur2015-pz,
  title     = "Bayesian estimation and inference",
  author    = "Zyphur, Michael J and Oswald, Frederick L",
  abstract  = "This paper introduces the ``Bayesian revolution'' that is
               sweeping across multiple disciplines but has yet to gain a
               foothold in organizational research. The foundations of Bayesian
               estimation and inference are first reviewed. Then, two empirical
               examples are provided to show how Bayesian methods can overcome
               limitations of frequentist methods: (a) a structural equation
               model of testosterone's effect on status in teams, where a
               Bayesian approach allows directly testing a traditional null
               hypothesis as a research hypothesis and allows estimating all
               possible residual covariances in a measurement model, neither of
               which are possible with frequentist methods; and (b) an
               ANOVA-style model from a true experiment of ego depletion's
               effects on performance, where Bayesian estimation with
               informative priors allows results from all previous research
               (via a meta-analysis and other previous studies) to be combined
               with estimates of study effects in a principled manner, yielding
               support for hypotheses that is not obtained with frequentist
               methods. Data are available from the first author, code for the
               program Mplus is provided, and tables illustrate how to present
               Bayesian results. In conclusion, the many benefits and few
               hindrances of Bayesian methods are discussed, where the major
               hindrance has been an easily solvable lack of familiarity by
               organizational researchers.",
  journal   = "J. Manage.",
  publisher = "SAGE Publications",
  volume    =  41,
  number    =  2,
  pages     = "390--420",
  month     =  feb,
  year      =  2015,
  language  = "en"
}

  @Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2023},
    url = {https://www.R-project.org/},
  }
